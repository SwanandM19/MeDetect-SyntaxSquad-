{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcef01d3",
   "metadata": {},
   "source": [
    "## Skin Disease Detection using Transfer Learning (ResNet152)\n",
    "- Dataset used: https://www.kaggle.com/datasets/ismailpromus/skin-diseases-image-dataset\n",
    "- Initially trained on Kaggle (GPU: T4 x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a542452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d07b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed to ensure constant results\n",
    "SEED = 42  \n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ab0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"ismailpromus/skin-diseases-image-dataset\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=sorted(os.listdir(os.path.join(path,'IMG_CLASSES')))\n",
    "\n",
    "class_index={name:i for i,name in enumerate(classes)} #dict so its easy to convert label to index later\n",
    "print(class_index, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae58052",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "BATCH_SIZE=32\n",
    "AUTOTUNE=tf.data.AUTOTUNE\n",
    "\n",
    "def process_image(file_path, label): #process image by converting it into an array of 224x224 size and normalize it\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image=tf.image.resize(image,(224,224))\n",
    "    image = preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "def make_dataset(data): #data=[(file_path,label_str)]\n",
    "  files= [f for f,_ in data]\n",
    "  labels=[class_index[l] for _,l in data]\n",
    "  ds=tf.data.Dataset.from_tensor_slices((files,labels))  #makes the tf dataset\n",
    "  ds=ds.map(process_image,num_parallel_calls=AUTOTUNE)  #map() calls the function process_image to each element in the tf dataset\n",
    "  ds=ds.shuffle(1000, seed=SEED).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_imgs=os.path.join(path, 'IMG_CLASSES')\n",
    "train_data=[] #holds (file_path,class_name)\n",
    "val_data=[]\n",
    "test_data=[]\n",
    "for classs in os.listdir(total_imgs):\n",
    "\n",
    "    images=os.listdir(os.path.join(total_imgs,classs))\n",
    "    train_num=int(0.8*len(images))\n",
    "    validation_num=int(0.1*len(images))\n",
    "    #these three lists hold the path_urls to the images within the class, 80% are testing rest \n",
    "    #are validation and test images\n",
    "    train_imgs= random.sample(images,train_num)\n",
    "    val_imgs=random.sample(list(set(images)-set(train_imgs)),validation_num)\n",
    "    test_imgs= list((set(images)-set(train_imgs)) - set(val_imgs))\n",
    "\n",
    "    for img_path in train_imgs:\n",
    "        # Join total_imgs with the image filename to get the full path\n",
    "        train_data.append((os.path.join(total_imgs, classs, img_path), classs))\n",
    "\n",
    "    for img_path in val_imgs:\n",
    "        # Join total_imgs with the image filename to get the full path\n",
    "        val_data.append((os.path.join(total_imgs, classs, img_path), classs))\n",
    "        \n",
    "    for img_path in test_imgs:\n",
    "        # Join total_imgs with the image filename to get the full path\n",
    "        test_data.append((os.path.join(total_imgs, classs, img_path), classs))\n",
    "\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(val_data)\n",
    "random.shuffle(test_data)\n",
    "train_ds=make_dataset(train_data)  #final tf.data.Dataset\n",
    "val_ds=make_dataset(val_data)\n",
    "test_ds=make_dataset(test_data)\n",
    "\n",
    "\n",
    "print(\"Number of training images:\", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c867dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator=train_ds.as_numpy_iterator()  #we have to convert to np iterator so we can take stuff from the tf dataset\n",
    "batch=data_iterator.next()\n",
    "print(batch[0].shape) #batch[0] is the 32 images\n",
    "print(batch[1].shape) #batch[1] is the 32 corresponding labels as a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.concatenate([y for x, y in train_ds], axis=0)\n",
    "weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = dict(enumerate(weights))\n",
    "print('Class weights: ')\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e46ec0",
   "metadata": {},
   "source": [
    "## Model Building and Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424994a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet152\n",
    "\n",
    "base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = True\n",
    "\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "num_classes = 10\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.35)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attempting with dropout(0.35), unfreezing 50 layers of resnet and lr tweaking with callback: ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', # The metric to watch\n",
    "                                 factor=0.2,       # Reduce LR by factor of 5 (1 * 0.2)\n",
    "                                 patience=2,       # Number of epochs with no improvement to wait\n",
    "                                 min_lr=1e-6,      # Don't let the LR go below this value\n",
    "                                 verbose=1)       # Print a message when LR is reduced\n",
    "\n",
    "\n",
    "#optimizer = Adam(learning_rate=0.001) lr default\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history=model.fit(train_ds,validation_data=val_ds,epochs=9,batch_size=BATCH_SIZE, callbacks=[lr_scheduler])\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e13bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and filename for saving the model\n",
    "\n",
    "# Saving the model in .h5 format.\n",
    "dir_name = \"enter_your_directory\"\n",
    "model_name = \"enter_model_name.h5\"\n",
    "\n",
    "# Verify if directory exists, if not then create it\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "# Save the model.\n",
    "model.save(os.path.join(dir_name, model_name))\n",
    "\n",
    "print(f\"Model saved to {os.path.join(dir_name, model_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fcb4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model in TFLITE format.\n",
    "\n",
    "# Path to your saved Keras model (.h5 file)\n",
    "h5_model_path = 'enter_your_director/enter_model_name.h5'\n",
    "\n",
    "# Load the saved Keras model\n",
    "model = tf.keras.models.load_model(h5_model_path)\n",
    "\n",
    "# Directory where you want to save the .tflite model\n",
    "tflite_dir = 'enter_your_directory/tflite_models'\n",
    "os.makedirs(tflite_dir, exist_ok=True)\n",
    "\n",
    "# Path for the converted .tflite file\n",
    "tflite_path = os.path.join(tflite_dir, 'enter_model_name.tflite')\n",
    "\n",
    "# Convert the Keras model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model to the specified path\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"TFLite model saved at: {tflite_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
